---
name: seedance-prompt
description: 'Build, validate, and optimise Seedance 2.0 prompts using the five-layer stack, @Tag delegation levels 1â€“4, quad-modal rules, and the JSON schema compiler. Use when constructing or debugging any T2V, I2V, V2V, or R2V prompt, or when output quality does not match the intended scene description.'
license: MIT
user-invocable: true
user-invokable: true
tags: ["prompt", "t2v", "i2v", "v2v", "r2v", "openclaw", "antigravity", "gemini-cli", "codex", "cursor"]
metadata: {"version": "3.4.0", "updated": "2026-02-25", "openclaw": {"emoji": "âœï¸", "homepage": "https://github.com/Emily2040/seedance-2.0"}, "parent": "seedance-20", "antigravity": {"emoji": "âœï¸", "homepage": "https://github.com/Emily2040/seedance-2.0"}, "gemini-cli": {"emoji": "âœï¸", "homepage": "https://github.com/Emily2040/seedance-2.0"}, "author": "Emily (@iamemily2050)", "repository": "https://github.com/Emily2040/seedance-2.0"}
---

# seedance-prompt

Prompt construction, @Tag reference system, JSON planning schema, and quad-modal protocol for Seedance 2.0.

## Scope

- Five-layer prompt structure (subject/action/camera/style/sound)
- Delegation levels 1â€“4 and when to use each
- @Tag role assignment and Universal Reference mode
- Quad-modal protocol rules and failure patterns
- JSON prompt schema and compile rules
- Prompt hygiene and anti-slop

## Out of scope

- Camera phrasing library â€” see [skill:seedance-camera]
- Character identity locking â€” see [skill:seedance-characters]
- VFX contracts â€” see [skill:seedance-vfx]
- Audio layers â€” see [skill:seedance-audio]

---

## The 6-Part Field Formula (cross-model validated)

From 10,000-generation practitioner data, this structure maps cleanly to the Five-Layer Stack:

```
[SHOT TYPE] + [SUBJECT] + [ACTION] + [STYLE] + [CAMERA MOVEMENT] + [AUDIO CUES]
```

This baseline works across thousands of generations. It is the field-validated form of the Five-Layer Stack.

> **Front-load rule**: The model weights early words more heavily. `"Beautiful woman dancing"` â‰  `"Woman, beautiful, dancing."` Order matters. Subject + action always first.

> **One action per prompt rule**: Multiple actions create AI confusion. `"Walking while talking while eating"` = chaos. One verb per shot.

---

## The Five-Layer Stack

Build prompts in this order. The model is motion-first; subject anchor before style.

```
1. SUBJECT  â€” who/what is central (identity anchor)
2. ACTION   â€” primary motion verb + physics/timing
3. CAMERA   â€” framing + movement + speed + angle
4. STYLE    â€” 1â€“3 tokens max (film language, not adjectives)
5. SOUND    â€” ambient + SFX + music + silence
+ CONSTRAINTS â€” what must stay consistent; what to avoid
```

First 20â€“30 words carry disproportionate weight. Subject + action always first.

---

## Delegation Levels

### Level 1 â€” Pure Intent (â‰¤30 words)

Use when the model knows the domain (food, brands, sports, everyday life).

```
ç”Ÿæˆä¸€ä¸ªç²¾ç¾é«˜çº§çš„å…°å·æ‹‰é¢å¹¿å‘Šï¼Œæ³¨æ„åˆ†é•œç¼–æ’
```

The model selects shots, music, pacing independently.

### Level 2 â€” Guided Direction (30â€“100 words)

Subject + action + environment + one camera note + one style anchor. Most common production mode.

### Level 3 â€” Time-Segmented (100â€“300 words)

Use explicit timestamps: `0â€“3s: ... 3â€“7s: ... 7â€“END: ...`

### Level 4 â€” Full Choreography (300â€“1000+ words)

Per-shot specifications. Use for fight scenes, lip-sync, product demos. See [skill:seedance-motion].

**Decision rule:** Does the model already know how to shoot this? Yes â†’ Level 1â€“2. Novel/precise â†’ Level 3â€“4.

---

## @Tag System

Entry modes:
- **First/Last Frame (é¦–å°¾å¸§):** One image + text. For simple I2V.
- **Universal Reference (å…¨èƒ½å‚è€ƒ):** Multi-modal. Use for everything else.

Input limits: Images Ã—9, Videos Ã—3, Audio Ã—3, Total files â‰¤12 (Rule of 12).

### Role assignment patterns

Every @Tag needs one explicit role. A bare tag is weak.

```
@Image1's character as the subject
@Image2 as the first frame / @Image3 as the last frame
Scene references @Image2
Wearing the outfit from @Image3
Reference @Video1's camera movement throughout
BGM references @Audio1
Voice timbre references @Audio1
Match the visual style of @Video1
```

### Reference vs. Edit (critical)

- `å‚è€ƒ@Video1çš„è¿é•œ` â†’ generate new content using Video1's technique
- `å°†@Video1ä¸­çš„äººç‰©æ¢æˆ...` â†’ modify Video1 directly

These trigger different model behaviors. Be explicit.

---

## Quad-Modal Rules

### T2V
- Subject + action first
- For known domains: Level 1â€“2, trust the model
- For novel content: Level 3â€“4 with explicit structure

### I2V
- `@Image1 as the first frame` for stability
- Add `@Image2 as the last frame` for motion-in-between
- Describe only the change from start image, not the whole scene

### V2V
- State what to keep and what to change
- Limit to 1â€“2 changes per generation
- Modes: reference (new content from technique) vs. edit (modify directly)

### R2V
- Each reference gets ONE job
- State all role assignments before any other content
- For multi-character: attribute every action by name

---

## JSON Schema v3

See [ref:json-schema] for complete schema, field reference, and compile function.

Minimal example (Level 2):

```json
{
  "v": "3.0",
  "meta": { "mode": "i2v", "level": 2, "dur": 10, "ar": "16:9", "res": "1080p" },
  "ref": { "char": "@Image1", "bg": "@Image2", "cam": "@Video1", "bgm": "@Audio1" },
  "shot": {
    "subj": "weathered woman, wool coat, rain platform",
    "act": "slow turn toward camera, breath misting",
    "cam": "dolly push MSâ†’CU over 8s",
    "light": "overhead practical, warm key, low-fill",
    "style": ["anamorphic", "grain", "muted"],
    "snd": { "amb": "rain bed", "sfx": ["train hum at 1s"], "mx": "piano at 2s" }
  },
  "lock": ["stable exposure", "no drift"],
  "exit": "hold 0.8s"
}
```

**Never paste JSON into Seedance.** JSON = plan. Compile to plain text before submitting.

---

## Prompt Hygiene

Delete these words â€” they are unmeasurable:
`cinematic` `epic` `masterpiece` `ultra-real` `award-winning` `stunning` `8K`

Replace with observable controls:

| âŒ | âœ… |
|---|---|
| cinematic lighting | single hard key 45Â° camera-left, warm amber, deep shadow |
| epic | wide shot, slow push-in, rising wind, low drone, crescendo at 6s |
| high quality | stable exposure, no flicker, clean edges |

**Conflict check** â€” never combine:
- locked-off + handheld
- bright flat + low-key shadows
- rapid cuts + long-take

---

## âš ï¸ Copyright & Content Policy

Full rules, substitution tables, and architecture/music/audio policy â†’ [skill:seedance-copyright]

**Summary hard blocks**: real celebrity faces Â· named franchise characters (Iron Man, Naruto, Mario) Â· named game characters Â· brand logos Â· copyrighted scene recreations Â· named musical compositions.

**Core rule**: describe the *look*, never the *name*.

| âŒ Blocked | âœ… Safe substitute |
|---|---|
| Iron Man | red-and-gold powered exoskeleton, chest reactor glow |
| Naruto | blond spiky-haired shinobi, orange jumpsuit, whisker scars |
| Batman | dark armoured vigilante, scalloped cape |
| Eiffel Tower night lights | glass lattice tower with illuminated night display |
| Bohemian Rhapsody as score | operatic rock build, piano into power chords, multi-voice choir |

For living real persons: never generate by name or distinctive likeness. Use archetype: `"tech billionaire in black T-shirt"` not `"Elon Musk"`.

---

## ğŸš« Anti-Slop Protocol

Full blacklist, decomposition patterns, and before/after repairs â†’ [skill:seedance-antislop]

**The one test**: *Can a camera, light meter, or stopwatch measure this word?* If no â†’ delete it.

**Instant-delete list**: `stunning` Â· `cinematic` Â· `epic` Â· `masterpiece` Â· `beautiful` Â· `breathtaking` Â· `8K` Â· `ultra-real` Â· `award-winning` Â· `immersive` Â· `ethereal` Â· `magical` Â· `otherworldly`

**Replace with observable controls**:

| âŒ Slop | âœ… Measurable |
|---|---|
| cinematic lighting | 45Â° hard key camera-left, amber gel, deep shadow camera-right |
| epic scene | wide shot, dolly pull reveals 200 soldiers, brass swell at 4 s |
| stunning sunset | warm backlight 3200K, 5 min post-horizon, long silhouette shadows |
| 8K ultra-real | stable exposure, no flicker, clean edges, no hallucinated geometry |
| ethereal forest | heavy fog, god rays through canopy, cool teal cast, dust motes |

---

## Routing

Copyright issues â†’ [skill:seedance-copyright]
Slop/quality audit â†’ [skill:seedance-antislop]
Camera phrasing â†’ [skill:seedance-camera]
Character identity â†’ [skill:seedance-characters]
VFX contracts â†’ [skill:seedance-vfx]
Audio layers â†’ [skill:seedance-audio]

---

## Agent gotchas

1. Place subject + action in the first 20â€“30 words. Everything else is secondary.
2. One primary motion verb per shot. Competing verbs = chaos.
3. Positive specification beats long negative lists.
4. Budget discipline: every word must be verifiable in-frame or in-audio.
5. For Level 1: the delegation command `æ³¨æ„åˆ†é•œç¼–æ’` activates director intelligence. Add it and step back.
6. **Audio is not optional.** Practitioners with 10,000+ generations confirm: ignoring audio produces flat results regardless of visual quality. Always specify ambient + SFX + music/silence decision.
7. **Seed discipline.** Seed control is available at API level (seed=-1 for random; set integer for reproducibility). On the web platform, seed configurability is not confirmed. When API launches: test seeds 1000â€“1010 on same prompt, build a typed seed library.
8. **First frame obsession.** Generate 10 variants of just the first frame. First-frame quality predicts entire video outcome. Select best, then build.
9. **Post-Feb-15 content gate.** Any named character (franchise, anime, game, streamer original) or named real person will trigger refusal or degraded output. Run [skill:seedance-copyright] check before submission.
